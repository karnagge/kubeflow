{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation for XGBoost Classification\n",
    "\n",
    "This notebook generates synthetic tabular data for demonstrating the end-to-end Kubeflow workflow with XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset\n",
    "\n",
    "We'll create a dataset with the following characteristics:\n",
    "- 10,000 samples\n",
    "- 20 features (15 informative, 5 redundant)\n",
    "- Binary classification\n",
    "- Some correlation between features to make it realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate synthetic data\n",
    "X, y = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=20,\n",
    "    n_informative=15,\n",
    "    n_redundant=5,\n",
    "    n_classes=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    class_sep=0.8,  # Make classes somewhat separable\n",
    "    flip_y=0.1      # Add some noise\n",
    ")\n",
    "\n",
    "# Create feature names\n",
    "feature_names = [f'feature_{i}' for i in range(20)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "# Display first few rows and basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "print(\"\\nClass distribution:\")\n",
    "display(df['target'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Some Missing Values\n",
    "\n",
    "To make the dataset more realistic, we'll introduce some missing values randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Add 5% missing values to some features\n",
    "features_with_missing = feature_names[:5]  # First 5 features will have missing values\n",
    "for feature in features_with_missing:\n",
    "    mask = np.random.random(len(df)) < 0.05  # 5% missing rate\n",
    "    df.loc[mask, feature] = np.nan\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# First split: separate test set\n",
    "train_val_data, test_data = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df['target']\n",
    ")\n",
    "\n",
    "# Second split: separate train and validation sets\n",
    "train_data, val_data = train_test_split(\n",
    "    train_val_data,\n",
    "    test_size=0.25,  # 0.25 x 0.8 = 0.2 of original data\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=train_val_data['target']\n",
    ")\n",
    "\n",
    "print(\"Dataset splits:\")\n",
    "print(f\"Train set: {len(train_data)} samples\")\n",
    "print(f\"Validation set: {len(val_data)} samples\")\n",
    "print(f\"Test set: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Data to Disk\n",
    "\n",
    "Save the datasets in CSV format for use in the preprocessing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create data directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../data/raw', exist_ok=True)\n",
    "\n",
    "# Save datasets\n",
    "train_data.to_csv('../data/raw/train.csv', index=False)\n",
    "val_data.to_csv('../data/raw/validation.csv', index=False)\n",
    "test_data.to_csv('../data/raw/test.csv', index=False)\n",
    "\n",
    "print(\"Datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "\n",
    "Save a description of the dataset for documentation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "data_description = {\n",
    "    'n_samples': len(df),\n",
    "    'n_features': len(feature_names),\n",
    "    'features': feature_names,\n",
    "    'target': 'Binary classification (0 or 1)',\n",
    "    'missing_values': features_with_missing,\n",
    "    'train_samples': len(train_data),\n",
    "    'val_samples': len(val_data),\n",
    "    'test_samples': len(test_data)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../data/raw/data_description.json', 'w') as f:\n",
    "    json.dump(data_description, f, indent=2)\n",
    "\n",
    "print(\"Data description saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}